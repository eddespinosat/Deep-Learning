# Deep-Learning

### 1. Effect deep generalization
The first program trains simple neural networks on the MNIST dataset to analyze how the hidden layer size affects the model's ability to generalize to new data. It uses various hidden layer sizes, evaluates performance on validation and test sets, and compares losses and accuracies based on the number of parameters. Finally, it visualizes the results in graphs.

---

### 2. Complexity Graphs
The second program works with the **German Credit Risk** dataset, implementing a neural network for binary classification (good/bad credit). It explores the impact of training set size and hidden layer size on training errors (\(E_{in}\)) and test errors (\(E_{out}\)). It performs repeated experiments, calculates error statistics, and shows graphs that relate errors to data size and model complexity.
